# Spectral analysis in Pioran

Here we only give a brief introduction on Gaussian processes and how we use them in `Pioran` to infer the power spectrum of a random time series. For more details on Gaussian processes, we refer the reader to the book by Rassmusen and Williams [^1].

## Gaussian process regression

A Gaussian process $\boldsymbol{f}$ is formally defined as a collection of random variables, any finite number of which have a joint Gaussian distribution. This process is fully described by its mean function $\mu(t)$ and its covariance function (or kernel) $k(t,t')$. The kernel must be a postive-definite function, i.e. the covariance matrix $K$ with elements $K_{ij} = k(t_i, t_j)$ must be positive-definite. 

#### Likelihood function

Assuming data $\boldsymbol{x}(t)$ are generated by a Gaussian process $\boldsymbol{f}$ with constant mean $\mu$ and covariance function $k(t,t')$, a likelihood function can be derived. The log-likelihood function is given by:

```math
\ln\mathcal{L}(\boldsymbol{\theta},\nu,\mu)=-\frac{1}{2}\left( \boldsymbol{x} - \mu \right)^{\rm T} \left(K +\nu\boldsymbol{\sigma^2} I\right)^{-1} \left( \boldsymbol{x} - \mu \right) -\dfrac{1}{2}\ln\left|K +\nu\boldsymbol{\sigma^2} I\right| - \dfrac{n}{2}\ln(2\pi)
```

where $\boldsymbol{\theta}$ are the parameters of the covariance function, $\nu$ is a scale on the measurement variance, $\mu$ is the mean of the process, $\boldsymbol{\sigma^2}$ is the measurement noise, and $n$ is the number of data points. In practice, Gaussian process regression is computationally expensive! The cost of evaluating this likelihood function scales as $\mathcal{O}(n^3)$ and the cost of storing the covariance matrix in the memory scales as $\mathcal{O}(n^2)$.

#### Conditional Gaussian process

The conditional distribution of the Gaussian process given the data  $\boldsymbol{x}$ and time values $\boldsymbol{t}$, is also a Gaussian process. This new distribution can be used to make predictions at new points $\boldsymbol{t_*}$ and the predictive distribution is given by

```math
\begin{aligned}
\mathbb{E}[\boldsymbol{f_*}| \boldsymbol{t},\boldsymbol{x},\boldsymbol{t_*}] &= K_* \left[K +\boldsymbol{\sigma^2} I \right]^{-1} \boldsymbol{x}.\\
\mathrm{Cov}[\boldsymbol{f_*}| \boldsymbol{t},\boldsymbol{x},\boldsymbol{t_*}] &= K_{**} - K_* \left[K +\boldsymbol{\sigma^2} I \right]^{-1} {K_*}^{\rm T}
\end{aligned}
```

where $K_*$ is the covariance matrix between the new points $\boldsymbol{t_*}$ and the old points $\boldsymbol{t}$, and $K_{**}$ is the covariance matrix between the new points $\boldsymbol{t_*}$. The cost of computing the conditional distribution scales as $\mathcal{O}(n^3)$. 

### Covariance functions and power spectral densities

Here we model time series data, i.e. data indexed by time. We will assume a one-dimensional and stationary Gaussian process. In this case, the covariance function $k(t,t')$ is only a function of the time separation $\tau = t - t'$. The covariance function and the power spectral density are Fourier pairs. 

```math
\mathcal{P}(f) = \int_{-\infty}^{+\infty} k(\tau) e^{-2{\rm i}\pi f \tau} {\rm d }\tau \quad  \quad k(\tau) = \int_{-\infty}^{+\infty}  \mathcal{P}(f) e^{2{\rm i}\pi f \tau} {\rm d }f
```
If we could use the power spectral density to compute the covariance function, we could then use the covariance function to compute the likelihood function and infer the parameters of the power spectral density using Gaussian process regression.
Unfortunately, it is not always easy to compute the covariance function from the power spectral density, for instance the [`DoubleBendingPowerLaw`](@ref) model does not have a known Fourier transform. 

## Gaussian processes in Pioran

In `Pioran`, we use Gaussian processes to model the time series data and infer the parameters of the power spectral density. We implement the [`ScalableGP`](@ref) type to build a Gaussian process using the [AbstractGPs](https://github.com/JuliaGaussianProcesses/AbstractGPs.jl) interface. 

As presented above, Gaussian process regression is expensive to compute and it is not possible to describe all possible spectral densities with the  analytical kernels. Here we propose to use an approximation of the power spectral density by a sum of simple power spectral densities. We choose these simple power spectral densities to be the Fourier transform of known covariance functions which have a quasi-separable structure. This enables the fast and scalable computation of the log-likelihood function using the `celerite` algorithm introduced by Foreman-Mackey et al.[^2].
Python implementations of the `celerite` algorithm are available in [celerite2](https://celerite2.readthedocs.io/en/latest/) and [tinygp](https://tinygp.readthedocs.io/en/stable/).

This method uses the quasi-separable structure of the covariance functions [`Celerite`](@ref), [`SHO`](@ref) and [`Exp`](@ref) to compute the log-likelihood in a linear scaling with the number of datapoints as shown in the following benchmarks.

![benchmark_likelihood](./figures/Likelihood_benchmarks_bis.png)


### Modelling the power spectral density

We use [`SingleBendingPowerLaw`](@ref) and [`DoubleBendingPowerLaw`](@ref) to model the power spectral density of the random process generating the time series data.

```@example modelling
using Plots
using Pioran 
ùìü1 = SingleBendingPowerLaw(1., .1, 3.4) 
ùìü2 = SingleBendingPowerLaw(.4, 1e-2, 3.)
ùìü3 = SingleBendingPowerLaw(.1, 3., 2.4)
ùìü1d = DoubleBendingPowerLaw(1.2,1e-3,2.4,1.1,4.2)
ùìü2d = DoubleBendingPowerLaw(0.2,1e-2,1.3,24.3,3.1)
ùìü3d = DoubleBendingPowerLaw(0.5,1e-3,2.1,91.2,4.9)
f = 10 .^ range(-4, stop=3, length=1000)
l = @layout [a b] 
p1 =  plot(f,[ùìü1(f),ùìü2(f),ùìü3(f)],xlabel="Frequency (day^-1)",ylabel="Power Spectral Density",legend=false,framestyle = :box,xscale=:log10,yscale=:log10,ylims=(1e-15,1e1),lw=2)
p2 =  plot(f,[ùìü1d(f),ùìü2d(f),ùìü3d(f)],xlabel="Frequency (day^-1)",legend=false,framestyle = :box,xscale=:log10,yscale=:log10,ylims=(1e-15,1e1),lw=2) 
plot(p1,p2,layout=l,size=(700,300),grid=false,left_margin=2Plots.mm,bottom_margin=20Plots.px,title=["Single bending power-law" "Double bending power-law"])
```

## Approximating the power spectral density

To obtain the covariance function, we approximate the power spectral density by a sum of basis functions $\psi(f)$. At the moment, we use either of the following basis functions:

```math
\begin{align}\begin{split}
\psi_4(f) = \frac{1}{1+f^4}\\
\psi_6(f) = \frac{1}{1+f^6}
\end{split}
\end{align}
```
These basis functions have analytical Fourier transforms and are used to approximate the covariance function. The Fourier transform of the basis functions are given by

```math
\begin{align}\begin{split}
\phi_4(\tau) &= \exp\left(-\pi\sqrt{2}\tau\right) \left(\cos\left(\pi\sqrt{2}\tau\right)+\sin\left(\pi\sqrt{2}\tau\right)\right)\\
\phi_6(\tau) &=\pi/3\exp{\left(-2\pi \tau\right)}+\exp{\left(-\pi\tau\right)}\left(\pi/3\cos\left(\pi\sqrt{3}\tau\right)+\pi/\sqrt{3}\sin\left(\pi\sqrt{3}\tau\right)\right)\end{split}
\end{align}
```

We need to specify the frequency range `f0` and `fM` over which the approximation is performed. We also need to specify the number of basis functions `J` to use. Once this is done the frequency grid is defined as:

```math
f_j=f_\mathrm{start}\left({f_\mathrm{stop}}/{f_\mathrm{start}}\right)^{j/(J-1)}
```

The approximation of the power spectral density is then given by

```math
\begin{align}
    \mathcal{P}(f)\simeq \tilde{\mathcal{P}}(f)&=  \sum\limits_{j=0}^{J-1} a_j \psi(f/f_j)\\
    \mathcal{R}(\tau)\simeq \tilde{\mathcal{R}}(\tau)&= \sum\limits_{j=0}^{J-1} a_j f_j \phi(\tau f_j)
\end{align}
```

Adding the constraint that the approximation and the true power spectrum must be equal on the grid of frequencies gives a linear system of $J$ equations for the coefficients $a_j$. This system can be written with a Toeplitz matrix $B$  and a vector $\boldsymbol{a}$ as:

```math
\begin{align}
\boldsymbol{p} = \boldsymbol{a} B \quad \text{where } B_{ij}=\psi(f_i/f_j) \text{ and } p_j = \mathcal{P}(f_j)
\end{align}
```

The values of $p_j$" are divided by $p_0$ so that the values of $a_j$ are not too high, what we are interested in is the amplitude of the covariance function which gives the variance of process - the integral of the power spectrum.

Visually, the approximation can be seen as follows:
```@example modelling
f0, fM = 1e-3, 1e3
ùìü = SingleBendingPowerLaw(.4, 1e-1, 3.)
f = 10 .^ range(-3, stop=3, length=1000)
psd_approx = Pioran.approximated_psd(f, ùìü, f0, fM, n_components=20,basis_function="SHO",individual=true)

plot(f,ùìü(f)/ùìü(f0),xscale=:log10,yscale=:log10,label="True PSD",xlabel="Frequency (day^-1)",ylabel="Power Spectral Density",lw=2,framestyle = :box,grid=false)
plot!(f,sum(psd_approx,dims=2),label="Approximated PSD",lw=2)
plot!(f,psd_approx,label=nothing,color=:black,alpha=.5,ylims=(1e-15,1e1),ls=:dot)
```

### Limitations and diagnostics for the approximation

!!! warning "Limitations of the approximation"
    This approximation is limited by the steepness of the basis functions, this means that if the power spectrum you want to approximate is steeper than the basis functions, the approximation may fail. Equivalently, the basis functions are flat at low frequencies, modelling a rising power spectrum at low frequencies can also be difficult.

In order to check the quality of the approximation, we can compute the residuals and ratios between the true and approximated power spectrum. First, we need to define the range of allowed values for each parameter to check. As we adopt a Bayesian workflow, one can use the prior distribution to define the range of allowed values for each parameter. This can be done as follows:

```@example modelling
using Distributions
using Random
rng = MersenneTwister(1234) 

min_f_b, max_f_b = 1e-3, 1e3
function prior_transform(cube)
    Œ±‚ÇÅ = quantile(Uniform(0.0, 1.25), cube[1])
    f‚ÇÅ = quantile(LogUniform(min_f_b, max_f_b), cube[2])
    Œ±‚ÇÇ = quantile(Uniform(Œ±‚ÇÅ, 4.0), cube[3])
    variance = quantile(LogNormal(-1,2), cube[4])
    return [Œ±‚ÇÅ, f‚ÇÅ, Œ±‚ÇÇ, variance]
end

P = 2000
unif = rand(rng, 4, P) 
priors = mapreduce(permutedims, hcat, [prior_transform(unif[:, i]) for i in 1:P]')
l = @layout [a b ; c d]
p1 = histogram(priors[1,:],xlabel="Œ±‚ÇÅ")
bins = 10.0 .^LinRange( log10(minimum(priors[2,:])),log10(quantile(priors[2,:],.99)),30)
p2 = histogram(priors[2,:],bins=bins,xaxis=(:log10,(bins[1],bins[end])),xlabel="f‚ÇÅ")
p3 = histogram(priors[3,:],xlabel="Œ±‚ÇÇ")
bins = 10.0 .^LinRange( log10(minimum(priors[4,:])),log10(quantile(priors[4,:],1)),30)
p4 = histogram(priors[4,:],xlabel="variance",bins=bins,xaxis=(:log10,(bins[1],bins[end])))
plot(p1,p2,p3,p4,layout=l,size=(700,300),grid=false,left_margin=2Plots.mm,bottom_margin=20Plots.px,legend=false)
```

We can then use the function [`run_diagnostics`](@ref) to assess the quality of the approximation. 
The first argument is an array containing the parameters of the power spectral density, the second argument is the variance of the process. `f_min` and `f_max` are the minimum and maximum frequencies of the time series, this is to show the window of observed frequencies in the plots.

```@example modelling
f_min, f_max = 1e-3 * 5, 1e3 / 5
figs = run_diagnostics(priors[1:3, :], priors[4, :], f0, fM, SingleBendingPowerLaw, f_min,f_max, n_components=20, basis_function="SHO")
```

The following plots are produced:
The mean of the residuals and ratios as a function of frequency.
```@raw html
<img src="./diagnostics_psd_approx.png" width="75%",alt="Mean of the residuals and ratios as a function of frequency."/>
```
The quantiles of the residuals and ratios as a function of frequency. 
```@raw html
<img src="./quantiles_psd_approx.png" width="75%" ,alt="Quantiles of the residuals and ratios as a function of frequency."/>
```
The distribution of the mean, median and maximum values of the frequency-averaged residuals and ratios.
```@raw html
<img src="./boxplot_psd_approx.png" width="75%" ,alt="Distribution of the mean, median and maximum values of the frequency-averaged residuals and ratios."/>
```
Using these three diagnostics we can see that a `SingleBendingPowerLaw` with parameters in the range of the chosen prior distribution can be approximated with a good accuracy.


## Building the Gaussian process

Now that we have checked that approximation hold for our choice of priors we can build the Gaussian process.

### Building the covariance function

The covariance function using the approximation of the power spectral density is obtained using the function [`approx`](@ref). We need to specify the frequency range `f0` and `fM` over which the approximation is performed, the number of basis functions `J` to use, and the variance of the process - integral of the power spectrum. One can also give the type of basis function to use, the default is `SHO` which corresponds to the basis function $\psi_4$, `DRWCelerite` corresponds to $\psi_6$.

```@example modelling
ùìü = SingleBendingPowerLaw(.4, 1e-1, 3.)

variance = 2.2
ùì° = approx(ùìü, f0, fM, 20, variance, basis_function="SHO")
```

### Building the Gaussian process

The Gaussian process is built using the type [`ScalableGP`](@ref). If the mean of the process $\mu$ is known, it can be given as a first argument. Otherwise, the mean is assumed to be zero.

```@example modelling
Œº = 1.3 
f = ScalableGP(Œº, ùì°)
```
At the moment, the GP does not include the measurement variance `œÉ¬≤` and the time values `t`. This is done in the next step.
```@example modelling
using DelimitedFiles # hide
A = readdlm("data/simu.txt",comments=true) # hide
t, y, yerr = A[:,1], A[:,2], A[:,3] # hide
œÉ¬≤ = yerr .^ 2 
fx = f(t, œÉ¬≤)
```
The log-likehood of the Gaussian process given the data `y` can be computed using the function `logpdf` from the `Distributions` package.
```@example modelling
logpdf(fx, y)
```

## References


[^1]: Rassmusen, C. E., & Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press.
[^2]: Foreman-Mackey D., Agol E., Ambikasaran S., Angus R., 2017, AJ, [154, 220](https://ui.adsabs.harvard.edu/abs/2017AJ....154..220F/abstract)